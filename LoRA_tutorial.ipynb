{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8ImOfN-3lzT"
   },
   "source": [
    "# ü¶æ Full Fine-Tuning VS LoRA\n",
    "\n",
    "First, we can start by importing the necesarry libraries. We can get our Tokenizer and SequenceClassification wrapper from Huggingface's transformers library.\n",
    "\n",
    "\n",
    "In essence, AutoModelForSequenceClassification has a classification head on top of the model outputs which can be easily trained with the base model\n",
    "\n",
    "For example, here's Qwen3-0.6B, the model we'll be fine-tuning today, wrapped in SequenceClassification:\n",
    "\n",
    "```\n",
    "Qwen3ForSequenceClassification(\n",
    "  (model): Qwen3Model(\n",
    "    (embed_tokens): Embedding(151936, 2048)\n",
    "    (layers): ModuleList(\n",
    "      (0-27): 28 x Qwen3DecoderLayer(\n",
    "        (self_attn): Qwen3Attention(\n",
    "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
    "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
    "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
    "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
    "        )\n",
    "        (mlp): Qwen3MLP(\n",
    "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
    "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
    "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
    "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
    "      )\n",
    "    )\n",
    "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
    "    (rotary_emb): Qwen3RotaryEmbedding()\n",
    "  )\n",
    "  (score): Linear(in_features=2048, out_features=2, bias=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlBGDuWER-vu"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U datasets\n",
    "!pip install -q -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YdamzuiWf9B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsihbPR8YBke"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET8xrMlL2_Ic"
   },
   "source": [
    "## üõ† Dataset Preprocessing\n",
    "\n",
    "Our dataset for this tutorial will be a simple Spam Email Classifier dataset from Kaggle (url: https://www.kaggle.com/datasets/sahideseker/spam-mail-classifier-dataset)\n",
    "\n",
    "The labels will be divided into \"ham\", which means a non-spam email (defined as 0), and \"spam, which means a spam email (defined as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t73yw5lhYHh1",
    "outputId": "e06790b8-f004-4d5a-8311-7ca3ec4b86d3"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d sahideseker/spam-mail-classifier-dataset\n",
    "!unzip spam-mail-classifier-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1ZCvINuhYIg0",
    "outputId": "4048a149-5684-4ff5-e586-fd78c8182844"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('spam_mail_classifier.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5Cv9WrigYbny",
    "outputId": "40185ab5-8cba-40a1-9e59-cd5baf847e65"
   },
   "outputs": [],
   "source": [
    "label_mapping = {\"ham\": 0, \"spam\": 1}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg7lBBOQ42tL"
   },
   "source": [
    "## ‚úÇ Dataset Split\n",
    "*   ### Train: 80%\n",
    "*   ### Val: 10%\n",
    "*   ### Test: 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QFWApktYfA0"
   },
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"email_text\"].tolist(),\n",
    "    df[\"label\"].tolist(),\n",
    "    test_size=0.1,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    test_size=0.1,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBd5bIVIYj0A"
   },
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "val_data = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOUnAzycdWnj",
    "outputId": "c1e0db8e-a6a7-4566-8b97-c43fef393ed9"
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGjEKbXxZI7u",
    "outputId": "2a693f15-bd1b-4cf1-8841-bf58c2036cf4"
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "efa732d72fd04e1288164c4126230de2",
      "84696cd1a0454111875de02e8b60527c",
      "8978459ef6484391ae88c6b249ba6def",
      "6952a0dd317147e69b7923ed0d86c3be",
      "2d956c3fc4524255b43d734a804ffdb9",
      "8e98b13e53bd41e3bd2a0f3592220c40",
      "433b473a574c4ee9a75fc34faaf2b5e3",
      "cd09818acca944979b3d20ae2e708bc2",
      "7a32aa99ffd0465db83d3977daeb8424",
      "93b123a32af842a797cf1b556fdda30d",
      "86318c9cceb74a91a74bcf0fbafe2046",
      "e42c25b6b1c0417f987eda01116c5381",
      "505284f604a24591818d41ed9d5735e2",
      "6f757f62e8bb4ce3bb5f7130dea837dd",
      "b82ea7604b2942faa901a8b6ac8065e5",
      "3719f4df845843f3917ed475d3f779d1",
      "cc9e93569ff04df19e22f304bb0dc100",
      "bd44fcb64a7e4e009615ccb7d82d9d48",
      "172a586add67439983e6d91e8863f74d",
      "fefa6a72714940c7b7d50364f01b63bf",
      "2939ca52314c4d42b7c857925cbd2de8",
      "b8b90419b08f4b02ac6379b06e25dd5e"
     ]
    },
    "id": "7kRnAnKaZgkH",
    "outputId": "e6b5dabe-0a9a-4c93-8423-7930d09977de"
   },
   "outputs": [],
   "source": [
    "def tokenize_func(example):\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_data = train_data.map(tokenize_func, batched=True)\n",
    "val_data = val_data.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0wOjw3Ba17p"
   },
   "outputs": [],
   "source": [
    "train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJNgDtgTa4G4"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYqWVh61bWtl"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average=\"binary\")\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2ApZsb32zAD"
   },
   "source": [
    "## üèÉ Training time\n",
    "\n",
    "Here we define out training arguments and set up our trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9ISss37rFsl",
    "outputId": "a0baff27-d82f-40fb-b092-871c52eede6d"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_fft\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./fft_logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "EVcaJSoObzHu",
    "outputId": "34c05611-8fdf-4f2e-8c4e-836502d04be2"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2z74Em-zzaS_"
   },
   "source": [
    "## üíæ Memory Usage During Full Fine-Tuning\n",
    "\n",
    "Peak Allocated: ~11.4 GB\n",
    "\n",
    "- This is the actual memory actively used by model parameters, activations, gradients, and optimizer states during training.\n",
    "\n",
    "Peak Reserved: ~12.3 GB\n",
    "\n",
    "- This includes extra memory reserved by PyTorch's memory allocator for internal buffers or future allocations.\n",
    "\n",
    "## ü§ì Observations:\n",
    "\n",
    "High peak allocated: ~11.4 GB\n",
    "- Indicates full fine-tuning is memory-intensive; all model weights, gradients, and optimizer states are stored and updated\n",
    "\n",
    "Reserved close to max: ~12.3 GB\n",
    "- PyTorch pre-allocates memory chunks to minimize fragmentation and speed up training\n",
    "\n",
    "GPU usage efficiency: High\n",
    "- Most of the reserved memory is being actively used, indicating efficient memory utilization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KmZr-BUGekvR",
    "outputId": "2ceba2f2-1f4d-43d0-a327-612a43f8de79"
   },
   "outputs": [],
   "source": [
    "print(f\"[Peak Allocated]: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"[Peak Reserved]:  {torch.cuda.max_memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "98c4f39e41714edab85afcbfeaae62e1",
      "4fa865596131423eb5d10ec270a8e5ba",
      "11c919c492854f3db29a31200b2db8eb",
      "84c8d1df550a46f591d44af9c8a28f0e",
      "73bbf6803b7b4a5eabc0dc02b6b4be90",
      "395e8267bb024dd0b8d0e36f65193a1c",
      "6edb089ea120442b8504cf8aaacbaae8",
      "58ab4d6b2fc04e3d9f0cbd0e9286a0ef",
      "b64f8b6dc8f441d7ab2dcdddbbdf2c93",
      "ddc072e4b06f47ed85818d213a18a833",
      "5786ca4ec3c544f99bef42290e84b84a"
     ]
    },
    "id": "moEhdVkPkBUV",
    "outputId": "de739e84-a224-40e2-b673-9893f2ffb3dc"
   },
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_dict({\n",
    "    \"text\": test_texts,\n",
    "    \"label\": test_labels\n",
    "})\n",
    "\n",
    "def tokenize_fn_test(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize_fn_test, batched=True)\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "bbZGw9A5kDsV",
    "outputId": "8e0301e4-fb09-4edb-9bf0-d314e98584c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "output = trainer.predict(test_dataset)\n",
    "preds = output.predictions.argmax(axis=-1)\n",
    "labels = output.label_ids\n",
    "\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UGUAb2-koX4",
    "outputId": "ded7cc93-e499-40d0-e84b-3f619fea1f45"
   },
   "outputs": [],
   "source": [
    "print(\"Fine-tuned Model Performance:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(labels, preds, target_names=[\"ham\", \"spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEXubtFIu0gr"
   },
   "source": [
    "# ü§ñ LoRA Finetuning Time\n",
    "\n",
    "First let's load in the model, tokenizer, and compute_metrics; basically the same as our full fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frL-XJnPlrnb",
    "outputId": "7c8608a1-2b5c-48bc-cf53-5ebb6f85527f"
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "3e324299f6bf4dd78e7592285a172bae",
      "9cef5517cc67408b80936d08b25b2243",
      "c6884afeb8664b45a7a19627439b9061",
      "65772e9a8709489d8e3e85061d68626b",
      "ae909fc98d6642e189fd28cd38daafcb",
      "5456f263c0d94f9faf3091beb84f425f",
      "edfa83241faf40a7abafbea3f6e8c613",
      "1487723eae81440c9677f29ca2b7b3a6",
      "97e1320cb2c24617a76d2ee718d0c541",
      "8938fbdf398848da9786e3931e818904",
      "1d255d9993154761a41ef1b5b4a3505e",
      "eb07bb99b0624141adf526fa1f35ec4e",
      "f247c886ac264989af24ceb3d135ca10",
      "4ab93c7e3b9a40ffa1d628de41f5bc92",
      "33042786f5054912bcedbd6fcb8d4e50",
      "b7ae8ac6e15347ae8321277ae108430b",
      "681dca5f22304d17817b24c30514e269",
      "bdf004c171fd4f439db677c572370845",
      "4cdf1147d9674e98b975d5d7424f0b7f",
      "dab838ebc8214e0ba691566d0280ae8b",
      "894c07810c8e4c1fa637e5c1ec0598b4",
      "30f64d2a6f1345a69f169fff57eb954a"
     ]
    },
    "id": "Bg2AZ0f_v5tk",
    "outputId": "4fa39968-07d6-45ea-fb53-668c23159ae8"
   },
   "outputs": [],
   "source": [
    "def tokenize_func(example):\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_data = train_data.map(tokenize_func, batched=True)\n",
    "val_data = val_data.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyopE2wuv_Un"
   },
   "outputs": [],
   "source": [
    "train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIbwIp01wCUu"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWH6bT0YwC1V"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average=\"binary\")\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjdEcFNPvOQI"
   },
   "source": [
    "## üìé LoRA's Rank and scaling factor Alpha\n",
    "\n",
    "In LoRA, instead of updating the full weight matrix $ W ‚àà ‚Ñù^{d√ók} $, we add a low-rank perturbation:\n",
    "\n",
    "\n",
    "$ \\Delta W = \\frac{\\alpha}{r} AB, \\quad A \\in \\mathbb{R}^{d \\times r}, \\quad B \\in \\mathbb{R}^{r \\times k} $\n",
    "\n",
    "- r (rank): Controls the capacity of the low-rank adaptation. Higher r = more expressive adapter.\n",
    "\n",
    "- Œ± (alpha): A scaling factor applied to the low-rank matrix. Best practice is $ \\alpha = 2r $, ensuring stable training and comparable gradient magnitudes across different ranks.\n",
    "\n",
    "\n",
    "## Impact on Performance\n",
    "- Lower ranks (e.g. r = 8-64): May underperform full fine-tuning on more complex tasks.\n",
    " - Can produce \"intruder dimensions\"  reference: https://arxiv.org/pdf/2410.21228, (can be mitigated with a good alpha, best practice Œ± = 2r)\n",
    "\n",
    "- Higher ranks (e.g. r = 256‚Äì1024): More closely approximate full fine-tuning performance.\n",
    " - Reduce or eliminate intruder dimensions, better preserve pretraining structure, and improve generalization  \n",
    "\n",
    "\n",
    "## Memory and Time\n",
    "Given weight matrix  $ W ‚àà ‚Ñù^{d√ók} $\n",
    "\n",
    "Let d = k = 4096. For a full matrix, training requires:\n",
    "\n",
    "- Full FT: 4096 √ó 4096 = 16M parameters.\n",
    "\n",
    "LoRA with r = 64 trains:\n",
    "- 4096 √ó 64 + 64 √ó 4096 = approx. 0.5M parameters (~3% of full).\n",
    "\n",
    "\n",
    "### TLDR:\n",
    "- Lower ranks 'forgets' less, has implicit regularization.\n",
    "- LoRA, with commonly used low-rank settings, underperforms full finetuning.\n",
    "- Higher ranks perform closer to full fine-tuning, but will use up more resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLcyPTdLv2rj"
   },
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x8pOO05wXCg"
   },
   "source": [
    "## ü§ì Observe the Model's architecture.\n",
    "\n",
    "We have several modules we can target for LoRA, namely:\n",
    "\n",
    "- q_proj: query projection\n",
    "- k_proj: key projection\n",
    "- v_proj: value projection\n",
    "- o_proj: output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8KczbEYwVg1",
    "outputId": "120c1b5c-69c1-48ee-d9b7-417b7ffccca9"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoC4bClI5zmJ"
   },
   "source": [
    "## üß∞ LoRA Config\n",
    "\n",
    "Here we can setup our LoRA configurations.\n",
    "\n",
    "In the config we can define our rank, lora_alpha, dropout, target_modules, etc.\n",
    "\n",
    "Currently, the best practice is to have lora_alpha = 2r, and target module be all the LinearLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42_p5YCBvY9j",
    "outputId": "a2b068a4-67eb-4028-dfd9-6b93d3fec001"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ae0krD6xehy"
   },
   "source": [
    "## ü§ì Observe the Attention layers\n",
    "\n",
    "Our model has been successfully wrapped with LoRA adapters using the peft library!\n",
    "\n",
    "We can see that we have indeed injected LoRA adapters into the following attention modules:\n",
    "\n",
    "- q_proj: query projection\n",
    "- k_proj: key projection\n",
    "- v_proj: value projection\n",
    "- o_proj: output projection\n",
    "\n",
    "Also observe the out_features of A and in_features of B, they follow the concept of LoRA, where\n",
    "\n",
    "\n",
    "$ \\Delta W = \\frac{\\alpha}{r} AB, \\quad A \\in \\mathbb{R}^{d \\times r}, \\quad B \\in \\mathbb{R}^{r \\times k} $\n",
    "\n",
    "- r (rank): Controls the capacity of the low-rank adaptation. Higher r = more expressive adapter.\n",
    "\n",
    "- Œ± (alpha): A scaling factor applied to the low-rank matrix. Best practice is $ \\alpha = 2r $, ensuring stable training and comparable gradient magnitudes across different ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzJOw8xlxbBq",
    "outputId": "673bf773-b1ec-4ce1-e7ce-bea71689c1f9"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SCn3dl8ydOd"
   },
   "source": [
    "## üèÉ Training time\n",
    "\n",
    "We'll be using the same training args to maintain consistency with our full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CakgEZUjxclj",
    "outputId": "7168b30f-9a25-497b-a41f-50eb3e266f8a"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_lora\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./lora_logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "o99OGL1LygnY",
    "outputId": "b2214dc9-90d8-48d5-b68a-3b90dafc3c9d"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO9FM-Fs0QG4"
   },
   "source": [
    "## üíæ Memory Usage Comparison: Full Fine-Tuning vs LoRA\n",
    "\n",
    "| Metric         | Full Fine-Tuning | LoRA Fine-Tuning |\n",
    "|----------------|------------------|------------------|\n",
    "| Peak Allocated | ~11.4 GB         | ~4.31 GB          |\n",
    "| Peak Reserved  | ~12.3 GB         | ~4.36 GB          |\n",
    "| Reduction      | -                | ~60%             |\n",
    "\n",
    "\n",
    "We saved quite a lot of memory! üòÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YPYJsHJyraZ",
    "outputId": "f9f9223b-95c0-4035-fa36-3c04988f96ca"
   },
   "outputs": [],
   "source": [
    "print(f\"[Peak Allocated]: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"[Peak Reserved]:  {torch.cuda.max_memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z06C_5wV2MMS"
   },
   "source": [
    "### üîç Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "78793f6ae21745ddbfe582dfcd25afc4",
      "3ac2515785234eec91d50ac215cb4bc3",
      "0168f9f82bfd461ca75cb656996d85fc",
      "de1d484d913c4abd8f6cc8bd500648e2",
      "016bfe6ca61f4fe594ce3315014a5c0d",
      "f08560e06fa240bc879a56a1d3f12c3e",
      "f6a95e557c904d88a5c8691d09a85ca9",
      "e79a3236e474437887e5556371d50ac6",
      "dbb48fd420bc45c68e51805993ca3f40",
      "fdfa24cb3c314b5288afe8d75810279e",
      "2839a7365753406d8c975451ab2f426e"
     ]
    },
    "id": "U_v95jUIywh6",
    "outputId": "3cea2f53-5812-44bc-dd31-4befa0b061b1"
   },
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_dict({\n",
    "    \"text\": test_texts,\n",
    "    \"label\": test_labels\n",
    "})\n",
    "\n",
    "def tokenize_fn_test(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize_fn_test, batched=True)\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Oaf2OGFqy31R",
    "outputId": "887386dd-4df1-4075-f200-440061c36743"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "output = trainer.predict(test_dataset)\n",
    "preds = output.predictions.argmax(axis=-1)\n",
    "labels = output.label_ids\n",
    "\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEsu6fva1d0B"
   },
   "source": [
    "## üéØ Performance Comparison\n",
    "\n",
    "Both full fine-tuning and LoRA achieved perfect accuracy on the test set. This is likely because the task‚Äîspam classification on this dataset‚Äîis relatively simple, and the model (Qwen3-0.6B) is strong enough to handle it with ease. Given the straightforward nature of the inputs and the limited size of the dataset, even lightweight fine-tuning methods like LoRA are sufficient to reach optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDSj7AAgy_rB",
    "outputId": "92fd10d7-ceb8-443b-e08a-562ad2dae951"
   },
   "outputs": [],
   "source": [
    "print(\"Fine-tuned Model Performance:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(labels, preds, target_names=[\"ham\", \"spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYVQ5qL7sPcN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
